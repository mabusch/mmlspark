{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 104 - Train, Test, Evaluate for Regression with Auto Imports Dataset\n",
    "\n",
    "This sample notebook is based on the Gallery [Sample 6: Train, Test, Evaluate for Regression: Auto Imports Dataset](https://gallery.cortanaintelligence.com/Experiment/670fbfc40c4f44438bfe72e47432ae7a) for AzureML Studio. This experiment demonstrates how to build a regression model to predict the aprice of automobile, based on various features. The process includes training, testing, and evaluating the model on the Automobile Imports data set.\n",
    "\n",
    "This sample demonstrates the use of several members of the mmlspark library:\n",
    "- `TrainRegressor`: [TrainRegressor](http://mmlspark.azureedge.net/docs/pyspark/TrainRegressor.html)\n",
    "- `SummarizeData`: [SummarizeData](http://mmlspark.azureedge.net/docs/pyspark/SummarizeData.html)\n",
    "- `CleanMissingData`: [CleanMissingData](http://mmlspark.azureedge.net/docs/pyspark/CleanMissingData.html)\n",
    "- `ComputeStatistics`: [ComputeStatistics](http://mmlspark.azureedge.net/docs/pyspark/ComputeStatistics.html)\n",
    "- `FindBestModel`: [FindBestModel](http://mmlspark.azureedge.net/docs/pyspark/FindBestModel.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import the pandas package so that we can read and parse the datafile using `pandas.read_csv()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare the schema for the data that will be converted from the pandas DataFrame to a Spark DataFrame. Allow all fields to be nullable, so that missing values can be handled appropriately, such as replacing them with the mean or median value for that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType, StringType, DoubleType, StructType, StructField\n",
    "\n",
    "colSchema = ((\"symboling\", LongType), (\"normalized-losses\", DoubleType), (\"make\", StringType), (\"fuel-type\", StringType),\n",
    "             (\"aspiration\", StringType), (\"body-style\", StringType), (\"drive-wheels\", StringType), (\"engine-location\", StringType),\n",
    "             (\"wheel-base\", DoubleType), (\"length\", DoubleType), (\"width\", DoubleType), (\"height\", DoubleType),\n",
    "             (\"curb-weight\", LongType), (\"engine-type\", StringType), (\"num-of-cylinders\", StringType), (\"engine-size\", LongType),\n",
    "             (\"fuel-system\", StringType), (\"bore\", DoubleType), (\"stroke\", DoubleType), (\"compression-ratio\", DoubleType),\n",
    "             (\"horsepower\", DoubleType), (\"peak-rpm\", DoubleType), (\"city-mpg\", LongType), (\"highway-mpg\", LongType),\n",
    "             (\"price\",DoubleType))\n",
    "\n",
    "tableSchema = StructType([StructField(column[0], column[1](),True) for column in colSchema])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Read the data from the AutomobilePriceRaw.csv file into a pandas dataframe. If the file has not already been retrieved and stored locally, construct the URL and retrieve the file to be stored locally. In `read_csv()`, specify possible reprsentations of missing values, and drop the 'num-of-doors' column as the data is read in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataFile = \"AutomobilePriceRaw.csv\"\n",
    "import os, urllib\n",
    "if not os.path.isfile(dataFile):\n",
    "    urllib.request.urlretrieve(\"https://mmlspark.azureedge.net/datasets/\"+dataFile, dataFile)\n",
    "data = spark.createDataFrame(pd.read_csv(dataFile,\n",
    "                                         na_values=['',' ','?'],\n",
    "                                         usecols=lambda x: x not in ['num-of-doors']), tableSchema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn more about the data that was just read into the DataFrame, summarize the data using `SummarizeData` and print the summary. For each column of the DataFrame, `SummarizeData` will report the summary statistics in the following subcategories for each column:\n",
    "* Feature name\n",
    "* Counts\n",
    "  * Count\n",
    "  * Unique Value Count\n",
    "  * Missing Value Count\n",
    "* Quantiles\n",
    "  * Min\n",
    "  * 1st Quartile\n",
    "  * Median\n",
    "  * 3rd Quartile\n",
    "  * Max\n",
    "* Sample Statistics\n",
    "  * Sample Variance\n",
    "  * Sample Standard Deviation\n",
    "  * Sample Skewness\n",
    "  * Sample Kurtosis\n",
    "* Percentiles\n",
    "  * P0.5\n",
    "  * P1\n",
    "  * P5\n",
    "  * P95\n",
    "  * P99\n",
    "  * P99.5\n",
    "  \n",
    "Note that several columns have missing values (normalized-losses, bore, stroke, horsepower, peak-rpm, price). This summary can be very useful during the initial phases of data discovery and characterization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmlspark import SummarizeData\n",
    "summary = SummarizeData().transform(data)\n",
    "summary.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into training and testing datasets\n",
    "train, test = data.randomSplit([0.6, 0.4], seed=123)\n",
    "train.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the `CleanMissingData` API to replace the missing values in the dataset with something more useful or meaningful. Specify a list of columns to be cleaned, and specify the corresponding output column names, which are not required to be the same as the input column names. `CleanMissiongData` offers the options of \"Mean\", \"Median\", or \"Custom\" for the replacement value. In the case of \"Custom\" value, the user also specifies the value to use via the \"customValue\" parameter. In this example, we will replace missing values in numeric columns with the median value for the column. We will define the model here, then use it as a Pipeline stage when we train our regression models and make our predictions in the following steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mmlspark import CleanMissingData\n",
    "cols = [\"normalized-losses\", \"stroke\", \"bore\", \"horsepower\", \"peak-rpm\", \"price\"]\n",
    "cleanModel = CleanMissingData().setCleaningMode(\"Median\").setInputCols(cols).setOutputCols(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create two Regressor models for comparison: Poisson Regression and Random Forest. Pyspark has implemented several regressors:\n",
    "* LinearRegression\n",
    "* IsotonicRegression\n",
    "* DecisionTreeRegressor\n",
    "* RandomForestRegressor\n",
    "* GBTRegressor (Gradient-Boosted Trees)\n",
    "* AFTSurvivalRegression (Accelerated Failure Time Model Survival)\n",
    "* GeneralizedLinearRegression - fit a generalized model by giving symbolic description of the linear preditor (link function) and a description of the error distribution (family). The following families are supported:\n",
    "  * Gaussian\n",
    "  * Binomial\n",
    "  * Poisson\n",
    "  * Gamma\n",
    "  * Tweedie - power link fundtion specified through \"linkPower\"\n",
    "Please refer to the [Pyspark API Documentation](http://spark.apache.org/docs/latest/api/python/) for more details.\n",
    "\n",
    "`TrainRegressor` creates a model based on the regressor and other parameters that are supplied to it, then trains data on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this next step, Create a Poisson Regression model using the `GeneralizedLinearRegressor` API from Spark and create a Pipeline using the `CleanMissingData` and `TrainRegressor` as pipeline stages to create and train the model. Note that because `TrainRegressor`exoects a \"labelCol\" to be set, there is no need to set \"linkPredictionCol\" when setting up the `GeneralizedLinearRegressor`. Fitting the pipe on the training dataset will train the model. Applying the `transform()` of the pipe to the test dataset creates the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train Poisson Regression Model\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from mmlspark import TrainRegressor\n",
    "\n",
    "glr = GeneralizedLinearRegression(family=\"poisson\", link=\"log\")\n",
    "poissonModel = TrainRegressor().setModel(glr).setLabelCol(\"price\").setNumFeatures(256)\n",
    "poissonPipe = Pipeline(stages = [cleanModel, poissonModel]).fit(train)\n",
    "poissonPrediction = poissonPipe.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, repeat these steps to create a Random Forest Regression model using the `RandomRorestRegressor` API from Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train Random Forest regression on the same training data:\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "rfr = RandomForestRegressor(maxDepth=30, maxBins=128, numTrees=8, minInstancesPerNode=1)\n",
    "randomForestModel = TrainRegressor(model=rfr, labelCol=\"price\", numFeatures=256).fit(train)\n",
    "randomForestPipe = Pipeline(stages = [cleanModel, randomForestModel]).fit(train)\n",
    "randomForestPrediction = randomForestPipe.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the models have been trained and scored, compute some basic statistics to evaluate the predictions. The following statistics are caluculated for regression models to evaluate:\n",
    "* Mean squared error\n",
    "* Root mean squared error\n",
    "* R^2\n",
    "* Mean absolute error\n",
    "\n",
    "Compute basic statistics for the Poisson model using the `ComputeModelStatistics` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ComputeStatistics to evaluate the PoissonRegressor:\n",
    "from mmlspark import ComputeModelStatistics\n",
    "poissonMetrics = ComputeModelStatistics().transform(poissonPrediction)\n",
    "print(\"Poisson Metrics\")\n",
    "poissonMetrics.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, compute the statistics for the Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ComputeStatistics to evaluate the RandomForestRegressor\n",
    "randomForestMetrics = ComputeModelStatistics().transform(randomForestPrediction)\n",
    "print(\"Random Forest Metrics\")\n",
    "randomForestMetrics.toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
